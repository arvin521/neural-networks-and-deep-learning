#!/usr/bin/env python
# -*- coding: utf-8 -*-

import mnist_loader
import network

if __name__ == "__main__":
    #先加载 MNIST 数据。
    training_data, validation_data, test_data = mnist_loader.load_data_wrapper()

    #设置一个有 30 个隐藏层神经元的 Network。我们在导入如上所列的名为 network 的 Python 程序后做
    net = network.Network([784, 30, 10])

    #使用随机梯度下降来从 MNIST training_data 学习超过 30 次迭代期,小批量数据大小为 10,学习速率 η = 3.0
    net.SGD(training_data, 30, 10, 3.0, test_data=test_data)

    '''
        一旦我们已经训练一个网络,它能在几乎任何的计算平台上快速的运行。例如,一旦我们给一个网络学会
    了一组好的权重集和偏置集,它能很容易地被移植到网络浏览器中以 Javascript 运行,或者在移动设备上的本地应用。
    在任何情况下,这是一个神经网络训练运行时的部分打印输出。打印内容显示了在每轮训练期后神经网络能正确识别测试图像的数量。
    正如你所⻅到,在仅仅一次迭代期后,达到了 10,000 中选中的 9,129 个。而且数目还在持续增⻓,
    Epoch 0: 9129 / 10000
    Epoch 1: 9295 / 10000
    Epoch 2: 9348 / 10000
    ...
    Epoch 27: 9528 / 10000
    Epoch 28: 9542 / 10000
    Epoch 29: 9534 / 10000

    更确切地说,经过训练的网络给出的识别率约为 95% —— 在峰值时为 95.42% (“Epoch 28”) !
    作为第一次尝试,这是非常令人鼓舞的。然而我应该提醒你,如果你运行代码然后得到的结果
    和我的不完全一样,那是因为我们使用了(不同的)随机权重和偏置来初始化我们的网络。我采
    用了三次运行中的最优结果作为本章的结果。

    让我们重新运行上面的实验,将隐藏神经元数量改到 100。正如前面的情况,如果你一边阅
    读一边运行代码,我应该警告你它将会花费相当⻓一段时间来执行(在我的机器上,这个实验
    每一轮训练迭代需要几十秒),因此比较明智的做法是当代码运行的同时,继续阅读。
    >>> net = network.Network([784, 100, 10])
    >>> net.SGD(training_data, 30, 10, 3.0, test_data=test_data)

    果然,它将结果提升至 96.59%。至少在这种情况下,使用更多的隐藏神经元帮助我们得到了更好的结果7。
    当然,为了获得这些准确性,我不得不对训练的迭代期数量,小批量数据大小和学习速率 η
    做特别的选择。正如我上面所提到的,这些在我们的神经网络中被称为超参数,以区别于通过
    我们的学习算法所学到的参数(权重和偏置)。如果我们选择了糟糕的超参数,我们会得到较差
    的结果。假如我们选定学习速率为 η = 0.001,
    >>> net = network.Network([784, 100, 10])
    >>> net.SGD(training_data, 30, 10, 0.001, test_data=test_data)

    结果则不太令人鼓舞了,
    Epoch 0: 1139 / 10000
    Epoch 1: 1136 / 10000
    Epoch 2: 1135 / 10000
    ...
    Epoch 27: 2101 / 10000
    Epoch 28: 2123 / 10000
    Epoch 29: 2142 / 10000

        然而,你可以看到网络的性能随着时间的推移慢慢地变好了。这表明应该增大学习速率,例
    如 η = 0.01。如果我们那样做了,我们会得到更好的结果,这表明我们应该再次增加 学习速    率。
    (如果改变能够改善一些事情,试着做更多!)如果我们这样做几次,我们最终会得到一个
    像 η = 1.0 的学习速率(或者调整到 3.0),这跟我们之前的实验很接近。因此即使我们最初选择
    了糟糕的超参数,我们至少获得了足够的信息来帮助我们改善对于超参数的选择。
    通常,调试一个神经网络是具有挑战性的。尤其是当初始的超参数的选择产生的结果还不如
    随机噪点的时候。假如我们试用之前成功的具有 30 个隐藏神经元的网络结构,但是学习速率改
    为 η = 100.0:

    >>> net = network.Network([784, 30, 10])
    >>> net.SGD(training_data, 30, 10, 100.0, test_data=test_data)

    在这点上,我们实际走的太远,学习速率太高了:
    Epoch 0: 1009 / 10000
    Epoch 1: 1009 / 10000
    Epoch 2: 1009 / 10000
    Epoch 3: 1009 / 10000
    ...
    Epoch 27: 982 / 10000
    Epoch 28: 982 / 10000
    Epoch 29: 982 / 10000

        现在想象一下,我们第一次遇到这样的问题。当然,我们从之前的实验中知道正确的做法是
    减小学习速率。但是如果我们第一次遇到这样的问题,那么输出的数据就不会有太多信息能指
    导我们怎么做。我们可能不仅关心学习速率,还要关心我们的神经网络中的其它每一个部分。我
    们可能想知道是否用了让网络很难学习的初始权重和偏置?或者可能我们没有足够的训练数据
    来获得有意义的学习?或者我们没有进行足够的迭代期?或者可能对于具有这种结构的神经网
    络,学习识别手写数字是不可能的?可能学习速率 太低?或者可能学习速率太高?当你第一次
    遇到问题,你不总是能有把握。
        从这得到的教训是调试一个神经网络不是琐碎的,就像常规编程那样,它是一⻔艺术。你需
    要学习调试的艺术来获得神经网络更好的结果。更普通的是,我们需要启发式方法来选择好的
    超参数和好的结构。我们将在整本书中讨论这些,包括上面我是怎么样选择超参数的。
    '''